{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Configuring project directory"
      ],
      "metadata": {
        "id": "gP80TErpMq8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SETUP: Work Locally but Import Existing Models ---\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Work locally\n",
        "os.chdir('/content/')\n",
        "\n",
        "# Copy existing trained models FROM Drive TO local\n",
        "drive_models_path = '/content/drive/MyDrive/589-mini-diffusion/MNISTMiniDiffusionResults' #change this to the location where existing models are\n",
        "existing_models = [\n",
        "    \"ddpm_mnist_baseline_epoch_50.pth\",\n",
        "    \"mnist_classifier.pth\",\n",
        "    \"ddpm_mnist_pruned_finetuned_epoch_20.pth\",\n",
        "    \"ddpm_mnist_reduced_channels_epoch_50.pth\",\n",
        "    \"ddpm_mnist_shallow_network_epoch_50.pth\",\n",
        "    \"ddpm_mnist_reduced_channels_finetuned_epoch_10.pth\",\n",
        "    \"ddpm_mnist_shallow_network_finetuned_epoch_20.pth\",\n",
        "]\n",
        "\n",
        "print(\"Copying existing models from Drive to local...\")\n",
        "for model_file in existing_models:\n",
        "    drive_path = f\"{drive_models_path}/{model_file}\"\n",
        "    if os.path.exists(drive_path):\n",
        "        shutil.copy2(drive_path, f\"/content/{model_file}\")\n",
        "        print(f\"Copied: {model_file}\")\n",
        "    else:\n",
        "        print(f\"Not found: {model_file} (will train from scratch)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz6lm4cGAr1H",
        "outputId": "00540469-e118-4102-b408-2361b4647681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Copying existing models from Drive to local...\n",
            "Copied: ddpm_mnist_baseline_epoch_50.pth\n",
            "Copied: mnist_classifier.pth\n",
            "Copied: ddpm_mnist_pruned_finetuned_epoch_20.pth\n",
            "Copied: ddpm_mnist_reduced_channels_epoch_50.pth\n",
            "Copied: ddpm_mnist_shallow_network_epoch_50.pth\n",
            "Copied: ddpm_mnist_reduced_channels_finetuned_epoch_10.pth\n",
            "Copied: ddpm_mnist_shallow_network_finetuned_epoch_20.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Installs"
      ],
      "metadata": {
        "id": "j3b4oxuT95DZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjNoTlS_BCfm",
        "outputId": "13c8f8d3-6741-4576-ba21-cc4f76a16fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.8.1-py3-none-any.whl (982 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m129.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.15.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 torchmetrics-1.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awjzWlTr9npp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "import gc\n",
        "import time\n",
        "import torch.nn.utils.prune as prune\n",
        "from torchmetrics.image import StructuralSimilarityIndexMeasure as SSIM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Model"
      ],
      "metadata": {
        "id": "fCe48mOR9xhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Configuration ---\n",
        "class Config:\n",
        "    image_size = 28\n",
        "    batch_size = 128\n",
        "    num_epochs = 50  # Baseline training epochs\n",
        "    fine_tune_epochs = 20 # Fine-tuning epochs after pruning\n",
        "    learning_rate = 1e-3\n",
        "    pruning_structured_amount = 0.1 # 10% structured (filter) pruning\n",
        "    pruning_unstructured_amount = 0.3 # 30% unstructured (weight) pruning\n",
        "    timesteps = 200\n",
        "    beta_start = 1e-4\n",
        "    beta_end = 0.02\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    num_generated_samples_for_eval = 10000 # Number of samples for FID/SSIM evaluation\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# --- 2. Data Preparation ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)) # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
        "\n",
        "# Create a DataLoader for the entire test set (real images) for evaluation metrics\n",
        "# This is needed for FID and SSIM comparison with generated images\n",
        "all_real_images = []\n",
        "for images, _ in test_loader:\n",
        "    all_real_images.append(images)\n",
        "all_real_images = torch.cat(all_real_images, dim=0).to(config.device)\n",
        "\n",
        "\n",
        "# --- 3. Diffusion Utilities (Noise Schedule) ---\n",
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "def linear_beta_schedule(timesteps, beta_start, beta_end):\n",
        "    return torch.linspace(beta_start, beta_end, timesteps)\n",
        "\n",
        "betas = cosine_beta_schedule(timesteps=config.timesteps).to(config.device)\n",
        "alphas = (1. - betas).to(config.device)\n",
        "alphas_cumprod = torch.cumprod(alphas, axis=0).to(config.device)\n",
        "alphas_cumprod_prev = torch.cat([torch.tensor([1.0], device=config.device), alphas_cumprod[:-1]]).to(config.device)\n",
        "\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).to(config.device)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod).to(config.device)\n",
        "\n",
        "posterior_variance = (betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)).to(config.device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def p_sample(model, x, t, t_index):\n",
        "    betas_t = betas[t_index]\n",
        "    sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod[t_index]\n",
        "    sqrt_alphas_t = sqrt_alphas_cumprod[t_index]\n",
        "\n",
        "    pred_noise = model(x, t)\n",
        "\n",
        "    pred_x0 = (x - pred_noise * sqrt_one_minus_alphas_cumprod_t) / sqrt_alphas_t\n",
        "\n",
        "    mean = (x - betas_t * pred_noise / sqrt_one_minus_alphas_cumprod_t) / torch.sqrt(alphas[t_index])\n",
        "\n",
        "    variance = posterior_variance[t_index]\n",
        "\n",
        "    noise = torch.randn_like(x) if t_index > 0 else 0.\n",
        "    return mean + torch.sqrt(variance) * noise\n",
        "\n",
        "@torch.no_grad()\n",
        "def p_sample_loop(model, shape):\n",
        "    img = torch.randn(shape, device=config.device)\n",
        "    for i in tqdm(reversed(range(0, config.timesteps)), desc='sampling loop time step', total=config.timesteps):\n",
        "        t = torch.full((img.shape[0],), i, device=config.device, dtype=torch.long)\n",
        "        img = p_sample(model, img, t, i)\n",
        "    return img\n",
        "\n",
        "@torch.no_grad()\n",
        "def p_sample_loop_chunked(model, total_samples, batch_size=100, image_shape=(1, 28, 28)):\n",
        "    \"\"\"Generate samples in clean chunks without corruption\"\"\"\n",
        "    model.eval()\n",
        "    all_samples = []\n",
        "\n",
        "    num_batches = (total_samples + batch_size - 1) // batch_size\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        current_batch_size = min(batch_size, total_samples - batch_idx * batch_size)\n",
        "\n",
        "        # Generate this batch independently\n",
        "        img = torch.randn((current_batch_size,) + image_shape, device=config.device)\n",
        "\n",
        "        for i in tqdm(reversed(range(0, config.timesteps)),\n",
        "                     desc=f'Batch {batch_idx+1}/{num_batches}',\n",
        "                     total=config.timesteps):\n",
        "            t = torch.full((img.shape[0],), i, device=config.device, dtype=torch.long)\n",
        "            img = p_sample(model, img, t, i)\n",
        "\n",
        "        all_samples.append(img)\n",
        "\n",
        "        print(f\"Completed batch {batch_idx+1}/{num_batches} ({current_batch_size} samples)\")\n",
        "\n",
        "        # Clear cache between batches\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Concatenate all batches\n",
        "    return torch.cat(all_samples, dim=0)\n",
        "\n",
        "def num_to_groups(num, divisor):\n",
        "    groups = num // divisor\n",
        "    remainder = num % divisor\n",
        "    arr = [divisor] * groups\n",
        "    if remainder > 0:\n",
        "        arr.append(remainder)\n",
        "    return arr\n",
        "\n",
        "# --- 4. Model Definition (Simplified U-Net) ---\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_emb_dim, out_channels)\n",
        "        )\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.residual_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
        "        else:\n",
        "            self.residual_conv = nn.Identity()\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        h = self.conv1(x)\n",
        "        h = self.bn1(h)\n",
        "        h = self.relu(h)\n",
        "        time_emb = self.mlp(t)\n",
        "        h = h + time_emb[:, :, None, None]\n",
        "        h = self.conv2(h)\n",
        "        h = self.bn2(h)\n",
        "        h = self.relu(h)\n",
        "        return h + self.residual_conv(x)\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, in_channels, 4, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose2d(in_channels, in_channels, 4, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "class SimpleUnet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 image_channels=1,\n",
        "                 channels=(32, 64, 128), # C0, C1, C2\n",
        "                 out_channels=1,\n",
        "                 time_emb_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
        "            nn.Linear(time_emb_dim, time_emb_dim),\n",
        "            nn.SiLU()\n",
        "        )\n",
        "\n",
        "        self.conv0 = nn.Conv2d(image_channels, channels[0], 3, padding=1)\n",
        "\n",
        "        self.downs = nn.ModuleList([])\n",
        "        for i in range(len(channels) - 1):\n",
        "            in_c = channels[i]\n",
        "            out_c = channels[i+1]\n",
        "            self.downs.append(nn.ModuleList([\n",
        "                Block(in_c, out_c, time_emb_dim),\n",
        "                Block(out_c, out_c, time_emb_dim),\n",
        "                Downsample(out_c)\n",
        "            ]))\n",
        "\n",
        "        self.mid_block1 = Block(channels[-1], channels[-1], time_emb_dim)\n",
        "        self.mid_block2 = Block(channels[-1], channels[-1], time_emb_dim)\n",
        "\n",
        "        self.ups = nn.ModuleList([])\n",
        "        for i in reversed(range(len(channels) - 1)):\n",
        "            current_level_channels = channels[i+1]\n",
        "            output_channels_for_this_stage = channels[i]\n",
        "\n",
        "            self.ups.append(nn.ModuleList([\n",
        "                Block(current_level_channels + current_level_channels, output_channels_for_this_stage, time_emb_dim),\n",
        "                Block(output_channels_for_this_stage, output_channels_for_this_stage, time_emb_dim),\n",
        "                Upsample(current_level_channels)\n",
        "            ]))\n",
        "\n",
        "        self.final_conv = nn.Conv2d(channels[0], out_channels, 1)\n",
        "\n",
        "    def forward(self, x, timestep):\n",
        "        t_emb = self.time_mlp(timestep)\n",
        "\n",
        "        x = self.conv0(x)\n",
        "        h = [] # Stores skip connections\n",
        "\n",
        "        # Downsampling\n",
        "        for block1, block2, downsample_layer in self.downs:\n",
        "            x = block1(x, t_emb)\n",
        "            x = block2(x, t_emb)\n",
        "            h.append(x)\n",
        "            x = downsample_layer(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.mid_block1(x, t_emb)\n",
        "        x = self.mid_block2(x, t_emb)\n",
        "\n",
        "        # Upsampling\n",
        "        for block1, block2, upsample_layer in self.ups:\n",
        "            skip_connection = h.pop()\n",
        "\n",
        "            x = upsample_layer(x)\n",
        "\n",
        "            x = torch.cat((x, skip_connection), dim=1)\n",
        "\n",
        "            x = block1(x, t_emb)\n",
        "            x = block2(x, t_emb)\n",
        "\n",
        "        return self.final_conv(x)\n",
        "\n",
        "# --- 5. Training/Fine-tuning Function ---\n",
        "def get_noise_pred(model, x_0, t):\n",
        "    sqrt_alphas_cumprod_t = sqrt_alphas_cumprod[t]\n",
        "    sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod[t]\n",
        "\n",
        "    noise = torch.randn_like(x_0)\n",
        "\n",
        "    x_t = sqrt_alphas_cumprod_t[:, None, None, None] * x_0 + \\\n",
        "          sqrt_one_minus_alphas_cumprod_t[:, None, None, None] * noise\n",
        "\n",
        "    predicted_noise = model(x_t, t.float())\n",
        "\n",
        "    return noise, predicted_noise\n",
        "\n",
        "def train_model(model, loader, num_epochs, learning_rate, model_save_path_prefix, is_finetuning=False):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    mode_str = \"Fine-tuning\" if is_finetuning else \"Training\"\n",
        "    print(f\"Starting {mode_str} on {config.device} for {num_epochs} epochs...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(loader, desc=f\"{mode_str} Epoch {epoch+1}/{num_epochs}\")\n",
        "        for batch_idx, (images, _) in enumerate(pbar):\n",
        "            images = images.to(config.device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            t = torch.randint(0, config.timesteps, (images.shape[0],), device=config.device).long()\n",
        "\n",
        "            noise, predicted_noise = get_noise_pred(model, images, t)\n",
        "\n",
        "            loss = criterion(predicted_noise, noise)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Corrected: Use full path for saving models\n",
        "        current_model_save_path = f\"{model_save_path_prefix}_epoch_{epoch+1}.pth\"\n",
        "        torch.save(model.state_dict(), current_model_save_path)\n",
        "        print(f\"Epoch {epoch+1} finished, Loss: {loss.item():.4f}. Model saved to {current_model_save_path}.\")\n",
        "\n",
        "        # Generate samples periodically\n",
        "        if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
        "            print(f\"Generating samples from {mode_str.lower()} model...\")\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                sample_batch_size = 16\n",
        "                generated_samples = p_sample_loop(model, shape=(sample_batch_size, 1, config.image_size, config.image_size))\n",
        "                final_samples_display = (generated_samples + 1) * 0.5 # Denormalize for display\n",
        "\n",
        "                # Corrected: Use full path for saving generated samples\n",
        "                current_samples_save_path = f\"{model_save_path_prefix}_generated_samples_epoch_{epoch+1}.png\"\n",
        "                torchvision.utils.save_image(final_samples_display, current_samples_save_path, nrow=4)\n",
        "                print(f\"Generated samples saved to {current_samples_save_path}.\")\n",
        "\n",
        "    print(f\"{mode_str} complete!\")"
      ],
      "metadata": {
        "id": "IjQKna4p9y4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74a625b2-3942-4ce2-ae50-f7ba6aa2a972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 39.4MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.03MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.39MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.77MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruning"
      ],
      "metadata": {
        "id": "sa0fryRA-Kxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_unstructured_pruning(model, amount=0.2):\n",
        "    \"\"\"\n",
        "    Applies unstructured pruning to all convolutional and linear layers\n",
        "    in the model.\n",
        "    \"\"\"\n",
        "    print(f\"\\nApplying unstructured pruning with amount: {amount}\")\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
        "            # Make pruning permanent (removes the reparameterization)\n",
        "            prune.remove(module, \"weight\")\n",
        "            # print(f\"  Applied unstructured pruning to: {name}\")\n",
        "\n",
        "    print(\"Unstructured pruning applied.\")\n",
        "\n",
        "def apply_structured_pruning(model, amount=0.2):\n",
        "    \"\"\"\n",
        "    Applies structured pruning (filter pruning) to Conv2d layers.\n",
        "    \"\"\"\n",
        "    print(f\"\\nApplying structured pruning with amount: {amount}\")\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            # dim=0 prunes output channels (filters)\n",
        "            prune.ln_structured(module, name=\"weight\", amount=amount, n=1, dim=0)\n",
        "            prune.remove(module, \"weight\")\n",
        "            # print(f\"  Applied structured pruning to: {name}\")\n",
        "\n",
        "    print(\"Structured pruning applied.\")"
      ],
      "metadata": {
        "id": "OWs-nMQZ-Mni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Architecture Modifications"
      ],
      "metadata": {
        "id": "joNkRCm9-Uad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_reduced_channels_model(train_loader, do_finetuning=False):\n",
        "    print(\"\\n--- Training Reduced Channels Model ---\")\n",
        "    print(\"Description: 50% reduction in channel dimensions\")\n",
        "    print(\"Channels: (16, 32, 64)\")\n",
        "    print(\"Time embedding dim: 128\")\n",
        "\n",
        "    model = SimpleUnet(\n",
        "        image_channels=1,\n",
        "        channels=(16, 32, 64),\n",
        "        out_channels=1,\n",
        "        time_emb_dim=128\n",
        "    ).to(config.device)\n",
        "\n",
        "    if do_finetuning:\n",
        "        initial_path = f\"ddpm_mnist_reduced_channels_epoch_{config.num_epochs}.pth\"\n",
        "        print(f\"Loading initial model from {initial_path} for fine-tuning\")\n",
        "        model.load_state_dict(torch.load(initial_path, map_location=config.device))\n",
        "\n",
        "        epochs = 10\n",
        "        lr = config.learning_rate / 100\n",
        "        model_path = f\"ddpm_mnist_reduced_channels_finetuned_epoch_{epochs}.pth\"\n",
        "        prefix = \"ddpm_mnist_reduced_channels_finetuned\"\n",
        "    else:\n",
        "        epochs = config.num_epochs\n",
        "        lr = config.learning_rate\n",
        "        model_path = f\"ddpm_mnist_reduced_channels_epoch_{epochs}.pth\"\n",
        "        prefix = \"ddpm_mnist_reduced_channels\"\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"Loading existing model from {model_path}\")\n",
        "        model.load_state_dict(torch.load(model_path, map_location=config.device))\n",
        "    else:\n",
        "        print(f\"Training reduced channels model...\")\n",
        "        train_model(model, train_loader, epochs, lr, prefix, is_finetuning=do_finetuning)\n",
        "        model.load_state_dict(torch.load(model_path, map_location=config.device))\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_shallow_network_model(train_loader, do_finetuning=False):\n",
        "    print(\"\\n--- Training Shallow Network Model ---\")\n",
        "    print(\"Description: Reduced network depth (2 levels vs 3)\")\n",
        "    print(\"Channels: (64, 128)\")\n",
        "    print(\"Time embedding dim: 256\")\n",
        "\n",
        "    model = SimpleUnet(\n",
        "        image_channels=1,\n",
        "        channels=(64, 128),\n",
        "        out_channels=1,\n",
        "        time_emb_dim=256\n",
        "    ).to(config.device)\n",
        "\n",
        "    if do_finetuning:\n",
        "        initial_path = f\"ddpm_mnist_shallow_network_epoch_{config.num_epochs}.pth\"\n",
        "        print(f\"Loading initial model from {initial_path} for fine-tuning\")\n",
        "        model.load_state_dict(torch.load(initial_path, map_location=config.device))\n",
        "\n",
        "        epochs = config.fine_tune_epochs\n",
        "        lr = config.learning_rate / 10\n",
        "        model_path = f\"ddpm_mnist_shallow_network_finetuned_epoch_{epochs}.pth\"\n",
        "        prefix = \"ddpm_mnist_shallow_network_finetuned\"\n",
        "    else:\n",
        "        epochs = config.num_epochs\n",
        "        lr = config.learning_rate\n",
        "        model_path = f\"ddpm_mnist_shallow_network_epoch_{epochs}.pth\"\n",
        "        prefix = \"ddpm_mnist_shallow_network\"\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"Loading existing model from {model_path}\")\n",
        "        model.load_state_dict(torch.load(model_path, map_location=config.device))\n",
        "    else:\n",
        "        print(f\"Training shallow network model...\")\n",
        "        train_model(model, train_loader, epochs, lr, prefix, is_finetuning=do_finetuning)\n",
        "        model.load_state_dict(torch.load(model_path, map_location=config.device))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "O3-KZ-tP-YNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "-RYZge9b_sDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to get number of parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Helper to get model size\n",
        "def get_model_size(model, filename_prefix=\"temp_model\"):\n",
        "    # Sanitize filename for model_name\n",
        "    sanitized_name = filename_prefix.lower().replace(' ', '_').replace('&', '').replace('-', '')\n",
        "    filename = f\"{sanitized_name}.pth\"\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
        "    os.remove(filename) # Clean up\n",
        "    return size_mb\n",
        "\n",
        "# --- MNIST Classifier for Pseudo-FID & Classification Accuracy ---\n",
        "class SimpleMNISTClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2), # 14x14\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2), # 7x7\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # Output 1x1\n",
        "        self.classifier = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def get_features(self, x):\n",
        "        return self.avgpool(self.features(x)).flatten(1) # Return flattened features\n",
        "\n",
        "def train_classifier(classifier_model, data_loader, num_epochs=10, lr=1e-3, device=\"cpu\"):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(classifier_model.parameters(), lr=lr)\n",
        "    classifier_model.to(device)\n",
        "    classifier_model.train()\n",
        "    print(f\"Training MNIST Classifier on {device}...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(data_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = classifier_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Classifier Epoch {epoch+1}, Loss: {running_loss / len(data_loader):.4f}\")\n",
        "    print(\"MNIST Classifier training complete.\")\n",
        "\n",
        "def evaluate_classifier_accuracy(classifier_model, images, labels):\n",
        "    classifier_model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = classifier_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total = labels.size(0)\n",
        "        correct = (predicted == labels).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "def calculate_pseudo_fid(classifier_model, real_images, generated_images, batch_size=128):\n",
        "    classifier_model.eval()\n",
        "    classifier_model.to(config.device)\n",
        "\n",
        "    all_real_features = []\n",
        "    all_gen_features = []\n",
        "\n",
        "    # Process real images\n",
        "    for i in tqdm(range(0, real_images.shape[0], batch_size), desc=\"Extracting real features\"):\n",
        "        batch = real_images[i:i + batch_size]\n",
        "        features = classifier_model.get_features(batch)\n",
        "        all_real_features.append(features.cpu())\n",
        "    real_features = torch.cat(all_real_features, dim=0)\n",
        "\n",
        "    # Process generated images\n",
        "    for i in tqdm(range(0, generated_images.shape[0], batch_size), desc=\"Extracting gen features\"):\n",
        "        batch = generated_images[i:i + batch_size]\n",
        "        features = classifier_model.get_features(batch)\n",
        "        all_gen_features.append(features.cpu())\n",
        "    gen_features = torch.cat(all_gen_features, dim=0)\n",
        "\n",
        "    # Calculate mean and covariance for real and generated features\n",
        "    mu_real, sigma_real = real_features.mean(dim=0), torch.cov(real_features.T)\n",
        "    mu_gen, sigma_gen = gen_features.mean(dim=0), torch.cov(gen_features.T)\n",
        "\n",
        "    # Add a small epsilon to the diagonal for numerical stability (to ensure positive definite)\n",
        "    eps = 1e-6 * torch.eye(sigma_real.shape[0], device=sigma_real.device)\n",
        "    sigma_real_reg = sigma_real + eps\n",
        "    sigma_gen_reg = sigma_gen + eps\n",
        "\n",
        "    # Calculate Frechet Distance using eigenvalue decomposition for matrix square root\n",
        "    # Based on: https://github.com/Lightning-AI/torchmetrics/blob/master/src/torchmetrics/image/fid.py#L225\n",
        "    # and the original FID paper\n",
        "    diff = mu_real - mu_gen\n",
        "\n",
        "    # Calculate matrix square root of (sigma_real @ sigma_gen)\n",
        "    # Using eigenvalue decomposition as torch.linalg.sqrtm might not be available\n",
        "    # For symmetric matrices, A = V @ D @ V.T, then sqrt(A) = V @ sqrt(D) @ V.T\n",
        "    cov_product = sigma_real_reg @ sigma_gen_reg\n",
        "\n",
        "    # Ensure symmetry for eigh\n",
        "    cov_product = (cov_product + cov_product.T) / 2.0\n",
        "\n",
        "    # Eigenvalue decomposition\n",
        "    eigvals, eigvecs = torch.linalg.eigh(cov_product)\n",
        "\n",
        "    # Filter out tiny negative eigenvalues and take square root\n",
        "    eigvals = torch.clamp(eigvals, min=0.0) # Ensure non-negative\n",
        "    sqrt_eigvals = torch.sqrt(eigvals)\n",
        "\n",
        "    # Construct the square root of the covariance product\n",
        "    cov_sqrt = eigvecs @ torch.diag_embed(sqrt_eigvals) @ eigvecs.T\n",
        "\n",
        "    # Handle potential complex numbers if `torch.linalg.sqrtm` returns them due to numerical issues\n",
        "    if cov_sqrt.is_complex():\n",
        "        cov_sqrt = cov_sqrt.real # Take only the real part if it's slightly off\n",
        "\n",
        "    fid_score = torch.sum(diff**2) + torch.trace(sigma_real + sigma_gen - 2 * cov_sqrt)\n",
        "\n",
        "    return fid_score.item()\n",
        "\n",
        "def evaluate_model(model, model_name, classifier_model, all_real_images, test_loader):\n",
        "    print(f\"\\n--- Evaluating {model_name} ---\")\n",
        "    model.eval()\n",
        "    model.to(config.device)\n",
        "\n",
        "    # 1. Model Efficiency Metrics\n",
        "    param_count = count_parameters(model)\n",
        "    print(f\"  Parameters: {param_count:,}\")\n",
        "\n",
        "    model_size_mb = get_model_size(model, model_name) # Pass model_name as prefix\n",
        "    print(f\"  Model Size: {model_size_mb:.2f} MB\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    generated_samples_eval = p_sample_loop(model, shape=(config.num_generated_samples_for_eval, 1, config.image_size, config.image_size))\n",
        "    # generated_samples_eval = p_sample_loop_chunked(model, config.num_generated_samples_for_eval, batch_size=100)\n",
        "    end_time = time.time()\n",
        "    inference_latency = end_time - start_time\n",
        "    print(f\"  Inference Latency ({config.num_generated_samples_for_eval} samples): {inference_latency:.4f} seconds\")\n",
        "\n",
        "    # Denormalize generated samples for SSIM/FID (expected range 0-1)\n",
        "    generated_samples_eval_denorm = (generated_samples_eval + 1) * 0.5\n",
        "\n",
        "    # Ensure all_real_images also matches the number of generated samples for fair comparison\n",
        "    # We take a random subset of real images if needed to match the num_generated_samples_for_eval\n",
        "    if all_real_images.shape[0] > config.num_generated_samples_for_eval:\n",
        "        indices = torch.randperm(all_real_images.shape[0])[:config.num_generated_samples_for_eval]\n",
        "        real_images_for_eval_denorm = (all_real_images[indices] + 1) * 0.5\n",
        "    else:\n",
        "        # If fewer real images than generated, use all real images and pad generated if necessary\n",
        "        # For simplicity, we assume num_generated_samples_for_eval <= total real images\n",
        "        real_images_for_eval_denorm = (all_real_images + 1) * 0.5\n",
        "\n",
        "\n",
        "    # 2. Image Quality Metrics\n",
        "    ssim_metric = SSIM(data_range=1.0, reduction='elementwise_mean').to(config.device)\n",
        "    ssim_score = ssim_metric(generated_samples_eval_denorm, real_images_for_eval_denorm).item()\n",
        "    print(f\"  Average SSIM: {ssim_score:.4f}\")\n",
        "\n",
        "    pseudo_fid_score = calculate_pseudo_fid(classifier_model, real_images_for_eval_denorm, generated_samples_eval_denorm)\n",
        "    print(f\"  Pseudo-FID: {pseudo_fid_score:.4f}\")\n",
        "\n",
        "    # 3. Accuracy (Classification Accuracy of generated digits)\n",
        "    # Evaluate classifier on generated samples\n",
        "    generated_labels_pred = classifier_model(generated_samples_eval).argmax(dim=1)\n",
        "    unique_classes, counts = torch.unique(generated_labels_pred, return_counts=True)\n",
        "    print(f\"  Generated Digit Distribution (from classifier): {dict(zip(unique_classes.tolist(), counts.tolist()))}\")\n",
        "\n",
        "    # Classifier accuracy on REAL MNIST test data (to confirm classifier itself is good)\n",
        "    test_images_for_classifier = torch.cat([images for images, _ in test_loader], dim=0).to(config.device)\n",
        "    test_labels_for_classifier = torch.cat([labels for _, labels in test_loader], dim=0).to(config.device)\n",
        "    classifier_test_accuracy = evaluate_classifier_accuracy(classifier_model, test_images_for_classifier, test_labels_for_classifier)\n",
        "    print(f\"  Classifier Accuracy on Real MNIST Test Set: {classifier_test_accuracy:.4f}\")\n",
        "\n",
        "    print(f\"--- Evaluation of {model_name} Complete ---\\n\")\n",
        "    return {\n",
        "        \"params\": param_count,\n",
        "        \"size_mb\": model_size_mb,\n",
        "        \"latency_s\": inference_latency,\n",
        "        \"ssim\": ssim_score,\n",
        "        \"pseudo_fid\": pseudo_fid_score,\n",
        "        \"classifier_test_accuracy\": classifier_test_accuracy\n",
        "    }"
      ],
      "metadata": {
        "id": "w9R5MIjXB_IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Full Execution Flow"
      ],
      "metadata": {
        "id": "Zgu8RcjcCepD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Train/Load MNIST Classifier ---\n",
        "mnist_classifier = SimpleMNISTClassifier().to(\"cpu\")\n",
        "classifier_path = \"mnist_classifier.pth\"\n",
        "\n",
        "if os.path.exists(classifier_path):\n",
        "    print(f\"Loading existing MNIST classifier from {classifier_path}\")\n",
        "    mnist_classifier.load_state_dict(torch.load(classifier_path, map_location=config.device))\n",
        "else:\n",
        "    print(\"Training MNIST Classifier (for evaluation metrics)...\")\n",
        "    classifier_train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "    train_classifier(mnist_classifier, classifier_train_loader, num_epochs=10, lr=1e-3, device=config.device)\n",
        "    torch.save(mnist_classifier.state_dict(), classifier_path)\n",
        "    print(f\"MNIST Classifier saved to {classifier_path}\")\n",
        "\n",
        "# --- Step 2: Train Baseline Model ---\n",
        "baseline_model = SimpleUnet(image_channels=1,\n",
        "                           channels=(32, 64, 128),\n",
        "                           out_channels=1,\n",
        "                           time_emb_dim=256).to(config.device)\n",
        "baseline_model_path = f\"ddpm_mnist_baseline_epoch_{config.num_epochs}.pth\"\n",
        "\n",
        "if os.path.exists(baseline_model_path):\n",
        "    print(f\"Loading existing baseline model from {baseline_model_path}\")\n",
        "    baseline_model.load_state_dict(torch.load(baseline_model_path, map_location=config.device))\n",
        "else:\n",
        "    print(\"Training baseline model...\")\n",
        "    train_model(baseline_model, train_loader, config.num_epochs, config.learning_rate, \"ddpm_mnist_baseline\")\n",
        "    baseline_model.load_state_dict(torch.load(baseline_model_path, map_location=config.device))\n",
        "\n",
        "# --- Step 3: Evaluate Baseline Model ---\n",
        "baseline_eval_results = evaluate_model(baseline_model, \"Baseline Model\", mnist_classifier, all_real_images, test_loader)\n",
        "del baseline_model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# --- Step 4: Train Architecture Variants ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING ARCHITECTURE VARIANTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Reduced Channels\n",
        "reduced_channels_finetuned_path = f\"ddpm_mnist_reduced_channels_finetuned_epoch_10.pth\"\n",
        "reduced_channels_initial_path = f\"ddpm_mnist_reduced_channels_epoch_{config.num_epochs}.pth\"\n",
        "\n",
        "if os.path.exists(reduced_channels_finetuned_path):\n",
        "    print(\"Loading existing fine-tuned reduced channels model...\")\n",
        "    reduced_channels_finetuned = train_reduced_channels_model(train_loader, do_finetuning=True)\n",
        "elif os.path.exists(reduced_channels_initial_path):\n",
        "    print(\"Initial model exists, fine-tuning reduced channels model...\")\n",
        "    reduced_channels_finetuned = train_reduced_channels_model(train_loader, do_finetuning=True)\n",
        "else:\n",
        "    print(\"Training initial then fine-tuning reduced channels model...\")\n",
        "    train_reduced_channels_model(train_loader)  # Train initial\n",
        "    reduced_channels_finetuned = train_reduced_channels_model(train_loader, do_finetuning=True)  # Fine-tune\n",
        "\n",
        "reduced_channels_finetuned_results = evaluate_model(reduced_channels_finetuned, \"Reduced Channels Fine-tuned\", mnist_classifier, all_real_images, test_loader)\n",
        "del reduced_channels_finetuned\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Shallow Network\n",
        "shallow_network_finetuned_path = f\"ddpm_mnist_shallow_network_finetuned_epoch_{config.fine_tune_epochs}.pth\"\n",
        "shallow_network_initial_path = f\"ddpm_mnist_shallow_network_epoch_{config.num_epochs}.pth\"\n",
        "\n",
        "if os.path.exists(shallow_network_finetuned_path):\n",
        "    print(\"Loading existing fine-tuned shallow network model...\")\n",
        "    shallow_network_finetuned = train_shallow_network_model(train_loader, do_finetuning=True)\n",
        "elif os.path.exists(shallow_network_initial_path):\n",
        "    print(\"Initial model exists, fine-tuning shallow network model...\")\n",
        "    shallow_network_finetuned = train_shallow_network_model(train_loader, do_finetuning=True)\n",
        "else:\n",
        "    print(\"Training initial then fine-tuning shallow network model...\")\n",
        "    train_shallow_network_model(train_loader)  # Train initial\n",
        "    shallow_network_finetuned = train_shallow_network_model(train_loader, do_finetuning=True)  # Fine-tune\n",
        "\n",
        "shallow_network_finetuned_results = evaluate_model(shallow_network_finetuned, \"Shallow Network Fine-tuned\", mnist_classifier, all_real_images, test_loader)\n",
        "del shallow_network_finetuned\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# --- Step 5: Prepare and Prune Model ---\n",
        "pruned_model = SimpleUnet(image_channels=1,\n",
        "                         channels=(32, 64, 128),\n",
        "                         out_channels=1,\n",
        "                         time_emb_dim=256).to(config.device)\n",
        "pruned_model.load_state_dict(torch.load(baseline_model_path, map_location=config.device))\n",
        "\n",
        "print(\"\\n--- Applying Pruning ---\")\n",
        "apply_structured_pruning(pruned_model, amount=config.pruning_structured_amount)\n",
        "apply_unstructured_pruning(pruned_model, amount=config.pruning_unstructured_amount)\n",
        "torch.save(pruned_model.state_dict(), \"ddpm_mnist_pruned_initial.pth\")\n",
        "print(\"Initial pruned model saved (before fine-tuning).\")\n",
        "\n",
        "# --- Step 7: Fine-tune Pruned Model ---\n",
        "pruned_model_path = f\"ddpm_mnist_pruned_finetuned_epoch_{config.fine_tune_epochs}.pth\"\n",
        "\n",
        "if os.path.exists(pruned_model_path):\n",
        "    print(f\"Loading existing pruned and fine-tuned model from {pruned_model_path}\")\n",
        "    pruned_model.load_state_dict(torch.load(pruned_model_path, map_location=config.device))\n",
        "else:\n",
        "    print(\"Fine-tuning pruned model...\")\n",
        "    fine_tune_lr = config.learning_rate / 10\n",
        "    train_model(pruned_model, train_loader, config.fine_tune_epochs, fine_tune_lr, \"ddpm_mnist_pruned_finetuned\", is_finetuning=True)\n",
        "    pruned_model.load_state_dict(torch.load(pruned_model_path, map_location=config.device))\n",
        "\n",
        "# --- Step 8: Evaluate Pruned and Fine-tuned Model ---\n",
        "pruned_eval_results = evaluate_model(pruned_model, \"Pruned & Fine-tuned Model\", mnist_classifier, all_real_images, test_loader)\n",
        "\n",
        "# --- Step 9: Summary of All Results ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPERIMENT SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "all_results = {\n",
        "    \"Baseline\": baseline_eval_results,\n",
        "    \"Reduced Channels\": reduced_channels_finetuned_results,\n",
        "    \"Shallow Network\": shallow_network_finetuned_results,\n",
        "    \"Pruned & Fine-tuned\": pruned_eval_results\n",
        "}\n",
        "\n",
        "print(\"Model Comparison Summary:\")\n",
        "print(f\"{'Model':<20} {'Pseudo-FID':<10} {'SSIM':<8} {'Params':<10} {'Size(MB)':<8}\")\n",
        "print(\"-\" * 70)\n",
        "for model_name, results in all_results.items():\n",
        "    print(f\"{model_name:<20} \"\n",
        "          f\"{results['pseudo_fid']:<10.2f} \"\n",
        "          f\"{results['ssim']:<8.4f} \"\n",
        "          f\"{results['params']:<10,} \"\n",
        "          f\"{results['size_mb']:<8.1f}\")"
      ],
      "metadata": {
        "id": "2OXh5dgl_txP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "571b49a6-6de1-4600-f9c5-91070f4f5977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing MNIST classifier from mnist_classifier.pth\n",
            "Loading existing baseline model from ddpm_mnist_baseline_epoch_50.pth\n",
            "\n",
            "--- Evaluating Baseline Model ---\n",
            "  Parameters: 2,529,217\n",
            "  Model Size: 9.72 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sampling loop time step: 100%|██████████| 200/200 [00:57<00:00,  3.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Inference Latency (10000 samples): 57.9701 seconds\n",
            "  Average SSIM: 0.3475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting real features: 100%|██████████| 79/79 [00:00<00:00, 1247.22it/s]\n",
            "Extracting gen features: 100%|██████████| 79/79 [00:00<00:00, 1489.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Pseudo-FID: 10.7524\n",
            "  Generated Digit Distribution (from classifier): {0: 1616, 1: 567, 2: 898, 3: 1009, 4: 760, 5: 873, 6: 1062, 7: 794, 8: 937, 9: 1484}\n",
            "  Classifier Accuracy on Real MNIST Test Set: 0.9787\n",
            "--- Evaluation of Baseline Model Complete ---\n",
            "\n",
            "\n",
            "================================================================================\n",
            "TRAINING ARCHITECTURE VARIANTS\n",
            "================================================================================\n",
            "Loading existing fine-tuned reduced channels model...\n",
            "\n",
            "--- Training Reduced Channels Model ---\n",
            "Description: 50% reduction in channel dimensions\n",
            "Channels: (16, 32, 64)\n",
            "Time embedding dim: 128\n",
            "Loading initial model from ddpm_mnist_reduced_channels_epoch_50.pth for fine-tuning\n",
            "Loading existing model from ddpm_mnist_reduced_channels_finetuned_epoch_10.pth\n",
            "\n",
            "--- Evaluating Reduced Channels Fine-tuned ---\n",
            "  Parameters: 634,081\n",
            "  Model Size: 2.49 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sampling loop time step: 100%|██████████| 200/200 [00:31<00:00,  6.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Inference Latency (10000 samples): 31.2825 seconds\n",
            "  Average SSIM: 0.3688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting real features: 100%|██████████| 79/79 [00:00<00:00, 1646.12it/s]\n",
            "Extracting gen features: 100%|██████████| 79/79 [00:00<00:00, 1779.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Pseudo-FID: 1.7628\n",
            "  Generated Digit Distribution (from classifier): {0: 1060, 1: 1169, 2: 967, 3: 1196, 4: 824, 5: 804, 6: 817, 7: 1127, 8: 820, 9: 1216}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Classifier Accuracy on Real MNIST Test Set: 0.9787\n",
            "--- Evaluation of Reduced Channels Fine-tuned Complete ---\n",
            "\n",
            "Loading existing fine-tuned shallow network model...\n",
            "\n",
            "--- Training Shallow Network Model ---\n",
            "Description: Reduced network depth (2 levels vs 3)\n",
            "Channels: (64, 128)\n",
            "Time embedding dim: 256\n",
            "Loading initial model from ddpm_mnist_shallow_network_epoch_50.pth for fine-tuning\n",
            "Loading existing model from ddpm_mnist_shallow_network_finetuned_epoch_20.pth\n",
            "\n",
            "--- Evaluating Shallow Network Fine-tuned ---\n",
            "  Parameters: 2,148,097\n",
            "  Model Size: 8.24 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sampling loop time step: 100%|██████████| 200/200 [01:21<00:00,  2.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Inference Latency (10000 samples): 81.5870 seconds\n",
            "  Average SSIM: 0.3659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting real features: 100%|██████████| 79/79 [00:00<00:00, 1939.78it/s]\n",
            "Extracting gen features: 100%|██████████| 79/79 [00:00<00:00, 1866.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Pseudo-FID: 2.3936\n",
            "  Generated Digit Distribution (from classifier): {0: 993, 1: 1113, 2: 856, 3: 1345, 4: 818, 5: 802, 6: 992, 7: 1185, 8: 750, 9: 1146}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Classifier Accuracy on Real MNIST Test Set: 0.9787\n",
            "--- Evaluation of Shallow Network Fine-tuned Complete ---\n",
            "\n",
            "\n",
            "--- Applying Pruning ---\n",
            "\n",
            "Applying structured pruning with amount: 0.1\n",
            "Structured pruning applied.\n",
            "\n",
            "Applying unstructured pruning with amount: 0.3\n",
            "Unstructured pruning applied.\n",
            "Initial pruned model saved (before fine-tuning).\n",
            "Loading existing pruned and fine-tuned model from ddpm_mnist_pruned_finetuned_epoch_20.pth\n",
            "\n",
            "--- Evaluating Pruned & Fine-tuned Model ---\n",
            "  Parameters: 2,529,217\n",
            "  Model Size: 9.72 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sampling loop time step: 100%|██████████| 200/200 [00:57<00:00,  3.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Inference Latency (10000 samples): 57.6236 seconds\n",
            "  Average SSIM: 0.3674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting real features: 100%|██████████| 79/79 [00:00<00:00, 1965.17it/s]\n",
            "Extracting gen features: 100%|██████████| 79/79 [00:00<00:00, 1595.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Pseudo-FID: 2.7686\n",
            "  Generated Digit Distribution (from classifier): {0: 837, 1: 1062, 2: 1139, 3: 1400, 4: 859, 5: 879, 6: 799, 7: 1001, 8: 875, 9: 1149}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Classifier Accuracy on Real MNIST Test Set: 0.9787\n",
            "--- Evaluation of Pruned & Fine-tuned Model Complete ---\n",
            "\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT SUMMARY\n",
            "================================================================================\n",
            "Model Comparison Summary:\n",
            "Model                Pseudo-FID SSIM     Params     Size(MB)\n",
            "----------------------------------------------------------------------\n",
            "Baseline             10.75      0.3475   2,529,217  9.7     \n",
            "Reduced Channels     1.76       0.3688   634,081    2.5     \n",
            "Shallow Network      2.39       0.3659   2,148,097  8.2     \n",
            "Pruned & Fine-tuned  2.77       0.3674   2,529,217  9.7     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "source_directory = '/content/'\n",
        "destination_directory = '/content/drive/MyDrive/589-project/'\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(destination_directory, exist_ok=True)\n",
        "print(f\"Destination directory ensured: {destination_directory}\")\n",
        "\n",
        "# 3. List the files you want to copy (adjust as needed)\n",
        "# You can list specific files or copy an entire directory\n",
        "files_to_copy = [\n",
        "    \"ddpm_mnist_baseline_epoch_50.pth\",\n",
        "    \"ddpm_mnist_baseline_generated_samples_epoch_10.png\",\n",
        "    \"ddpm_mnist_baseline_generated_samples_epoch_20.png\",\n",
        "    \"ddpm_mnist_baseline_generated_samples_epoch_30.png\",\n",
        "    \"ddpm_mnist_baseline_generated_samples_epoch_40.png\",\n",
        "    \"ddpm_mnist_baseline_generated_samples_epoch_50.png\",\n",
        "    \"ddpm_mnist_pruned_initial.pth\",\n",
        "    \"ddpm_mnist_pruned_finetuned_epoch_20.pth\",\n",
        "    \"ddpm_mnist_pruned_finetuned_generated_samples_epoch_10.png\",\n",
        "    \"ddpm_mnist_pruned_finetuned_generated_samples_epoch_20.png\",\n",
        "    \"ddpm_mnist_reduced_channels_epoch_50.pth\",\n",
        "    \"ddpm_mnist_reduced_channels_finetuned_epoch_10.pth\",\n",
        "    \"ddpm_mnist_reduced_channels_generated_samples_epoch_10.png\",\n",
        "    \"ddpm_mnist_reduced_channels_generated_samples_epoch_20.png\",\n",
        "    \"ddpm_mnist_reduced_channels_generated_samples_epoch_30.png\",\n",
        "    \"ddpm_mnist_reduced_channels_generated_samples_epoch_40.png\",\n",
        "    \"ddpm_mnist_reduced_channels_generated_samples_epoch_50.png\",\n",
        "    \"ddpm_mnist_reduced_channels_finetuned_generated_samples_epoch_10.png\",\n",
        "    \"ddpm_mnist_shallow_network_epoch_50.pth\",\n",
        "    \"ddpm_mnist_shallow_network_generated_samples_epoch_10.png\",\n",
        "    \"ddpm_mnist_shallow_network_generated_samples_epoch_20.png\",\n",
        "    \"ddpm_mnist_shallow_network_generated_samples_epoch_30.png\",\n",
        "    \"ddpm_mnist_shallow_network_generated_samples_epoch_40.png\",\n",
        "    \"ddpm_mnist_shallow_network_generated_samples_epoch_50.png\",\n",
        "    \"ddpm_mnist_shallow_network_finetuned_epoch_20.pth\",\n",
        "    \"ddpm_mnist_shallow_network_finetuned_generated_samples_epoch_10.png\",\n",
        "    \"ddpm_mnist_shallow_network_finetuned_generated_samples_epoch_20.png\",\n",
        "    \"mnist_classifier.pth\"\n",
        "]\n",
        "\n",
        "print(\"\\nCopying files to Google Drive...\")\n",
        "for filename in files_to_copy:\n",
        "    source_path = os.path.join(source_directory, filename)\n",
        "    destination_path = os.path.join(destination_directory, filename)\n",
        "\n",
        "    if os.path.exists(source_path):\n",
        "        try:\n",
        "            shutil.copy2(source_path, destination_path)\n",
        "            print(f\"Copied '{filename}' to '{destination_directory}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error copying '{filename}': {e}\")\n",
        "    else:\n",
        "        print(f\"Warning: Source file '{filename}' not found at '{source_path}'\")\n",
        "\n",
        "print(\"\\nCopying complete!\")"
      ],
      "metadata": {
        "id": "bO_GY7mDPhYS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5810c6f6-c71f-404e-8f25-901ce97002f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Destination directory ensured: /content/drive/MyDrive/589-project/\n",
            "\n",
            "Copying files to Google Drive...\n",
            "Copied 'ddpm_mnist_baseline_epoch_50.pth' to '/content/drive/MyDrive/589-project/'\n",
            "Warning: Source file 'ddpm_mnist_baseline_generated_samples_epoch_10.png' not found at '/content/ddpm_mnist_baseline_generated_samples_epoch_10.png'\n",
            "Warning: Source file 'ddpm_mnist_baseline_generated_samples_epoch_20.png' not found at '/content/ddpm_mnist_baseline_generated_samples_epoch_20.png'\n",
            "Warning: Source file 'ddpm_mnist_baseline_generated_samples_epoch_30.png' not found at '/content/ddpm_mnist_baseline_generated_samples_epoch_30.png'\n",
            "Warning: Source file 'ddpm_mnist_baseline_generated_samples_epoch_40.png' not found at '/content/ddpm_mnist_baseline_generated_samples_epoch_40.png'\n",
            "Warning: Source file 'ddpm_mnist_baseline_generated_samples_epoch_50.png' not found at '/content/ddpm_mnist_baseline_generated_samples_epoch_50.png'\n",
            "Copied 'ddpm_mnist_pruned_initial.pth' to '/content/drive/MyDrive/589-project/'\n",
            "Copied 'ddpm_mnist_pruned_finetuned_epoch_20.pth' to '/content/drive/MyDrive/589-project/'\n",
            "Warning: Source file 'ddpm_mnist_pruned_finetuned_generated_samples_epoch_10.png' not found at '/content/ddpm_mnist_pruned_finetuned_generated_samples_epoch_10.png'\n",
            "Warning: Source file 'ddpm_mnist_pruned_finetuned_generated_samples_epoch_20.png' not found at '/content/ddpm_mnist_pruned_finetuned_generated_samples_epoch_20.png'\n",
            "Copied 'ddpm_mnist_reduced_channels_epoch_50.pth' to '/content/drive/MyDrive/589-project/'\n",
            "Copied 'ddpm_mnist_reduced_channels_finetuned_epoch_10.pth' to '/content/drive/MyDrive/589-project/'\n",
            "Warning: Source file 'ddpm_mnist_reduced_channels_generated_samples_epoch_10.png' not found at '/content/ddpm_mnist_reduced_channels_generated_samples_epoch_10.png'\n",
            "Warning: Source file 'ddpm_mnist_reduced_channels_generated_samples_epoch_20.png' not found at '/content/ddpm_mnist_reduced_channels_generated_samples_epoch_20.png'\n",
            "Warning: Source file 'ddpm_mnist_reduced_channels_generated_samples_epoch_30.png' not found at '/content/ddpm_mnist_reduced_channels_generated_samples_epoch_30.png'\n",
            "Warning: Source file 'ddpm_mnist_reduced_channels_generated_samples_epoch_40.png' not found at '/content/ddpm_mnist_reduced_channels_generated_samples_epoch_40.png'\n",
            "Warning: Source file 'ddpm_mnist_reduced_channels_generated_samples_epoch_50.png' not found at '/content/ddpm_mnist_reduced_channels_generated_samples_epoch_50.png'\n",
            "Warning: Source file 'ddpm_mnist_reduced_channels_finetuned_generated_samples_epoch_10.png' not found at '/content/ddpm_mnist_reduced_channels_finetuned_generated_samples_epoch_10.png'\n",
            "Copied 'ddpm_mnist_shallow_network_epoch_50.pth' to '/content/drive/MyDrive/589-project/'\n",
            "Warning: Source file 'ddpm_mnist_shallow_network_generated_samples_epoch_10.png' not found at '/content/ddpm_mnist_shallow_network_generated_samples_epoch_10.png'\n",
            "Warning: Source file 'ddpm_mnist_shallow_network_generated_samples_epoch_20.png' not found at '/content/ddpm_mnist_shallow_network_generated_samples_epoch_20.png'\n",
            "Warning: Source file 'ddpm_mnist_shallow_network_generated_samples_epoch_30.png' not found at '/content/ddpm_mnist_shallow_network_generated_samples_epoch_30.png'\n",
            "Warning: Source file 'ddpm_mnist_shallow_network_generated_samples_epoch_40.png' not found at '/content/ddpm_mnist_shallow_network_generated_samples_epoch_40.png'\n",
            "Warning: Source file 'ddpm_mnist_shallow_network_generated_samples_epoch_50.png' not found at '/content/ddpm_mnist_shallow_network_generated_samples_epoch_50.png'\n",
            "Copied 'ddpm_mnist_shallow_network_finetuned_epoch_20.pth' to '/content/drive/MyDrive/589-project/'\n",
            "Warning: Source file 'ddpm_mnist_shallow_network_finetuned_generated_samples_epoch_10.png' not found at '/content/ddpm_mnist_shallow_network_finetuned_generated_samples_epoch_10.png'\n",
            "Warning: Source file 'ddpm_mnist_shallow_network_finetuned_generated_samples_epoch_20.png' not found at '/content/ddpm_mnist_shallow_network_finetuned_generated_samples_epoch_20.png'\n",
            "Copied 'mnist_classifier.pth' to '/content/drive/MyDrive/589-project/'\n",
            "\n",
            "Copying complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TFpjM3TOT4LT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}